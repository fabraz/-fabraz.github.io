[
  {
    "objectID": "posts/nlp_basics/nlp_basics.html",
    "href": "posts/nlp_basics/nlp_basics.html",
    "title": "Why can‚Äôt I use text like it is for NLP AI modeling?",
    "section": "",
    "text": "Last Friday I was in a clubhouse conversation aiming to explore groundbreaking ‚ÄúAttention is all that you need‚Äù paper when someone asked: why can‚Äôt I just use the text like it is to feed the neural net to modeling?\nSpeakers start to turn around and the question IMHO wasn‚Äôt answered. If you do not figure that, maybe multi-head attention either. That‚Äôs my motivation to tackle this subject.\nLet start with a very common task in NLP: sentiment classification. Moreover, why not begin with a rule-based approach for this challenge?\nFirst, we can define negative and positive word lists (find lot more here).\n\n\n\nPositive Word Cloud\n\n\n\n\n\nNegative Word Cloud\n\n\nNow with input text, you have to count occurrences of the words in the list, sum up the number for negative and positive separately, taking big result as the winner sentiment. Here are some examples.\n\n\n\nCounter Table\n\n\nHave you realized that we just moved from char/word symbolic space to number space? The sum word occurrences for each sentiment is the only source considered to decide between positive or negative.\nThe starting point of any NLP experiment is the definition of the language vocabulary, which means the only terms your solution will be aware of. The positive and negative lists merged into the vocabulary considered by the rule-based approach shown.\nUsing the machine learning approach, things change heavily since the rule we did to determine the sentence sentiment is now learned by the model in the training phase. At that time the model goal is to define a math function that generalizes the skill for defining the sentiment according to the words in a sentence (bag of words - BoW).\n\n\n\nLabel samples\n\n\nAs well as we did previously, the flow requires the language vocabulary used. The model will test equations (i.e.: w.x + b, linear model) varing their coeficients (w and b) in order to address the expected link between the sentence and its label. Mostly models, neural nets included, cannot deal with alpha symbols. That‚Äôs why we need to express the sentence content in terms of numbers (i.e.¬†BoW, TFIDF, etc).\nThe sentence The good, the bad and the ugly about interviewing users gives the following words to vocabulary:\n\nfrom nltk.tokenize import word_tokenize\nsentence = \"The good, the bad and the ugly about interviewing users\"\nprint(sorted(set(word_tokenize(sentence))))\n\n[',', 'The', 'about', 'and', 'bad', 'good', 'interviewing', 'the', 'ugly', 'users']\n\n\nAs you may note, the term the is repeated. Picture how many possibilities would have for it:\n\ndef case_combinations(word):\n    if len(word) == 0:\n        return []\n    elif len(word) == 1:\n        return [word.lower(), word.upper()]\n    else:\n        rest = case_combinations(word[1:])\n        return [word[0].lower() + x for x in rest] + [word[0].upper() + x for x in rest]\ncase_combinations('the')\n\n['the', 'thE', 'tHe', 'tHE', 'The', 'ThE', 'THe', 'THE']\n\n\nWhat about ‚Äúinterviewing‚Äù? 4096 ways to write üò±. Would not have enough computational power to deal with the 5% of 600,000 English words like that. That‚Äôs the reason for the normalization of terms before machine learning modeling.\nConsidering only the six samples presented bellow, we would have the following vocabulary:\n\nsentences = [\"She shook her head to clear the ugly thought.\",\n             \"The almost worst thing that can happen is you'll make an ugly or unusable candle that you'll have to melt and start over.\",\n            \"The good, the bad and the ugly about interviewing users\",\n            \"The real gift of gratitude is that the more grateful you are, the more present you become\",\n            \"When we focus on our gratitude, the tide of disappointment goes out and the tide of love rushes in.\",\n            \"Now and then it‚Äôs good to pause in our pursuit of happiness and just be happy.\"]\nvocabulary = sorted(set(word_tokenize(' '.join(sentences).lower())))\nprint(vocabulary)\n\n[\"'ll\", ',', '.', 'about', 'almost', 'an', 'and', 'are', 'bad', 'be', 'become', 'can', 'candle', 'clear', 'disappointment', 'focus', 'gift', 'goes', 'good', 'grateful', 'gratitude', 'happen', 'happiness', 'happy', 'have', 'head', 'her', 'in', 'interviewing', 'is', 'it', 'just', 'love', 'make', 'melt', 'more', 'now', 'of', 'on', 'or', 'our', 'out', 'over', 'pause', 'present', 'pursuit', 'real', 'rushes', 's', 'she', 'shook', 'start', 'that', 'the', 'then', 'thing', 'thought', 'tide', 'to', 'ugly', 'unusable', 'users', 'we', 'when', 'worst', 'you', '‚Äô']\n\n\nWith the vocabulary in hand, we finally can get the proper input data for modeling, pictured by a Pandas Dataframe below, where I‚Äôve highlighted some values greater than zero, which means the sample has the word. For example, sample 3 ‚ÄúNow and then it‚Äôs good to pause in our pursuit of happiness and just be happy.‚Äù has 2 occurrences of term and beyond other terms.\n\nimport pandas as pd\nimport numpy as np\nsamples = [[sentence.split().count(word) for word in vocabulary] for sentence in sentences] \ndf = pd.DataFrame(np.array(samples),columns=vocabulary)\ndf\n\n\n\n\n\n\n\n\n'll\n,\n.\nabout\nalmost\nan\nand\nare\nbad\nbe\n...\ntide\nto\nugly\nunusable\nusers\nwe\nwhen\nworst\nyou\n‚Äô\n\n\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n1\n1\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n0\n1\n1\n1\n0\n0\n0\n...\n0\n1\n1\n1\n0\n0\n0\n1\n0\n0\n\n\n2\n0\n0\n0\n1\n0\n0\n1\n0\n1\n0\n...\n0\n0\n1\n0\n1\n0\n0\n0\n0\n0\n\n\n3\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n2\n0\n\n\n4\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n...\n2\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\n5\n0\n0\n0\n0\n0\n0\n2\n0\n0\n1\n...\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n6 rows √ó 67 columns\n\n\n\nThere are many approaches for describing text numerically. Depending on the model some of them are inappropriate. It‚Äôs what happened between transformers models and BoW. You may find a lot more ways also to pre-process text data. Let me know how have you been preparing your text data.\nThanks for reading!\nBest!\nFB"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "blog",
    "section": "",
    "text": "Why can‚Äôt I use text like it is for NLP AI modeling?\n\n\n\n\n\n\n\nnlp\n\n\nbasics\n\n\n\n\n\n\n\n\n\n\n\nFeb 22, 2021\n\n\nFabricio Braz\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  }
]
{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Why can't I use text like it is for NLP AI modeling?\"\n",
    "author: \"Fabricio Braz\"\n",
    "date: \"2021-02-22\"\n",
    "categories: [nlp, basics]\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last Friday I was in a clubhouse conversation aiming to explore groundbreaking ‚ÄúAttention is all that you need‚Äù paper when someone asked: why can‚Äôt I just use the text like it is to feed the neural net to modeling?\n",
    "\n",
    "Speakers start to turn around and the question IMHO wasn‚Äôt answered. If you do not figure that, maybe multi-head attention either. That‚Äôs my motivation to tackle this subject.\n",
    "\n",
    "Let start with a very common task in NLP: sentiment classification. Moreover, why not begin with a rule-based approach for this challenge?\n",
    "\n",
    "First, we can define negative and positive word lists (find lot more [here](https://github.com/jeffreybreen/twitter-sentiment-analysis-tutorial-201107/tree/master/data/opinion-lexicon-English))."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#| label: positive_wc \n",
    "#| fig-cap: Positive word cloud\n",
    "![Positive Word Cloud](2023-04-01-17-11-00.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#| label: negative_wc\n",
    "![Negative Word Cloud](2023-04-01-17-09-02.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with input text, you have to count occurrences of the words in the list, sum up the number for negative and positive separately, taking big result as the winner sentiment. Here are some examples."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#| label: couter_table\n",
    "#| fig-cap: Counter negative vs Positive words (removed words without ocurrence from list)\n",
    "![Counter Table](2023-04-01-17-08-11.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have you realized that we just moved from char/word symbolic space to number space? The sum word occurrences for each sentiment is the only source considered to decide between positive or negative.\n",
    "\n",
    "The starting point of any NLP experiment is the definition of the `language vocabulary`, which means the only terms your solution will be aware of. The positive and negative lists merged into the vocabulary considered by the rule-based approach shown.\n",
    "\n",
    "Using the `machine learning` approach, things change heavily since the rule we did to determine the sentence sentiment is now learned by the model in the training phase. At that time the model goal is to define a math function that generalizes the skill for defining the sentiment according to the words in a sentence (`bag of words - BoW`)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#| label: label_table\n",
    "![ Label samples](2023-04-01-17-14-09.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "As well as we did previously, the flow requires the language vocabulary used. The model will test equations (i.e.: w.x + b, linear model) varing their coeficients (w and b) in order to address the expected link between the sentence and its label. **Mostly models, neural nets included, cannot deal with alpha symbols. That's why we need to express the sentence content in terms of numbers** (i.e. BoW, TFIDF, etc).\n",
    "\n",
    "The sentence `The good, the bad and the ugly about interviewing users` gives the following words to vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[',', 'The', 'about', 'and', 'bad', 'good', 'interviewing', 'the', 'ugly', 'users']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "sentence = \"The good, the bad and the ugly about interviewing users\"\n",
    "print(sorted(set(word_tokenize(sentence))))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you may note, the term `the` is repeated. Picture how many possibilities would have for it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'thE', 'tHe', 'tHE', 'The', 'ThE', 'THe', 'THE']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def case_combinations(word):\n",
    "    if len(word) == 0:\n",
    "        return []\n",
    "    elif len(word) == 1:\n",
    "        return [word.lower(), word.upper()]\n",
    "    else:\n",
    "        rest = case_combinations(word[1:])\n",
    "        return [word[0].lower() + x for x in rest] + [word[0].upper() + x for x in rest]\n",
    "case_combinations('the')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about ‚Äúinterviewing‚Äù? 4096 ways to write üò±. Would not have enough computational power to deal with the 5% of 600,000 English words like that. That‚Äôs the reason for the normalization of terms before machine learning modeling.\n",
    "\n",
    "Considering only the six samples presented bellow, we would have the following vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'ll\", ',', '.', 'about', 'almost', 'an', 'and', 'are', 'bad', 'be', 'become', 'can', 'candle', 'clear', 'disappointment', 'focus', 'gift', 'goes', 'good', 'grateful', 'gratitude', 'happen', 'happiness', 'happy', 'have', 'head', 'her', 'in', 'interviewing', 'is', 'it', 'just', 'love', 'make', 'melt', 'more', 'now', 'of', 'on', 'or', 'our', 'out', 'over', 'pause', 'present', 'pursuit', 'real', 'rushes', 's', 'she', 'shook', 'start', 'that', 'the', 'then', 'thing', 'thought', 'tide', 'to', 'ugly', 'unusable', 'users', 'we', 'when', 'worst', 'you', '‚Äô']\n"
     ]
    }
   ],
   "source": [
    "sentences = [\"She shook her head to clear the ugly thought.\",\n",
    "             \"The almost worst thing that can happen is you'll make an ugly or unusable candle that you'll have to melt and start over.\",\n",
    "            \"The good, the bad and the ugly about interviewing users\",\n",
    "            \"The real gift of gratitude is that the more grateful you are, the more present you become\",\n",
    "            \"When we focus on our gratitude, the tide of disappointment goes out and the tide of love rushes in.\",\n",
    "            \"Now and then it‚Äôs good to pause in our pursuit of happiness and just be happy.\"]\n",
    "vocabulary = sorted(set(word_tokenize(' '.join(sentences).lower())))\n",
    "print(vocabulary)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the vocabulary in hand, we finally can get the proper input data for modeling, pictured by a Pandas Dataframe below, where I‚Äôve highlighted some values greater than zero, which means the sample has the word. For example, sample 3 ‚ÄúNow and then it‚Äôs good to pause in our pursuit of happiness and just be happy.‚Äù has 2 occurrences of term `and` beyond other terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>'ll</th>\n",
       "      <th>,</th>\n",
       "      <th>.</th>\n",
       "      <th>about</th>\n",
       "      <th>almost</th>\n",
       "      <th>an</th>\n",
       "      <th>and</th>\n",
       "      <th>are</th>\n",
       "      <th>bad</th>\n",
       "      <th>be</th>\n",
       "      <th>...</th>\n",
       "      <th>tide</th>\n",
       "      <th>to</th>\n",
       "      <th>ugly</th>\n",
       "      <th>unusable</th>\n",
       "      <th>users</th>\n",
       "      <th>we</th>\n",
       "      <th>when</th>\n",
       "      <th>worst</th>\n",
       "      <th>you</th>\n",
       "      <th>‚Äô</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows √ó 67 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   'll  ,  .  about  almost  an  and  are  bad  be  ...  tide  to  ugly  \\\n",
       "0    0  0  0      0       0   0    0    0    0   0  ...     0   1     1   \n",
       "1    0  0  0      0       1   1    1    0    0   0  ...     0   1     1   \n",
       "2    0  0  0      1       0   0    1    0    1   0  ...     0   0     1   \n",
       "3    0  0  0      0       0   0    0    0    0   0  ...     0   0     0   \n",
       "4    0  0  0      0       0   0    1    0    0   0  ...     2   0     0   \n",
       "5    0  0  0      0       0   0    2    0    0   1  ...     0   1     0   \n",
       "\n",
       "   unusable  users  we  when  worst  you  ‚Äô  \n",
       "0         0      0   0     0      0    0  0  \n",
       "1         1      0   0     0      1    0  0  \n",
       "2         0      1   0     0      0    0  0  \n",
       "3         0      0   0     0      0    2  0  \n",
       "4         0      0   1     0      0    0  0  \n",
       "5         0      0   0     0      0    0  0  \n",
       "\n",
       "[6 rows x 67 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "samples = [[sentence.split().count(word) for word in vocabulary] for sentence in sentences] \n",
    "df = pd.DataFrame(np.array(samples),columns=vocabulary)\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many approaches for describing text numerically. Depending on the model some of them are inappropriate. It‚Äôs what happened between `transformers` models and BoW. You may find a lot more ways also to pre-process text data. `Let me know how have you been preparing your text data`.\n",
    "\n",
    "Thanks for reading!\n",
    "\n",
    "Best!\n",
    "\n",
    "FB"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
